% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model-ebm.R
\name{gcm}
\alias{gcm}
\alias{ebm_j}
\alias{ebm}
\title{Exemplar-based Cognitive Models}
\usage{
gcm(
  formula,
  class,
  data,
  fix = NULL,
  options = NULL,
  similarity = "minkowski",
  ...
)

ebm_j(
  formula,
  criterion,
  data,
  fix = NULL,
  options = NULL,
  similarity = "minkowski",
  ...
)

ebm(formula, criterion, data, mode, fix = NULL, options = NULL, ...)
}
\arguments{
\item{formula}{A \href{stats::formula}{formula}, the variables in \code{data} to be modeled. For example, \code{y ~ x1 + x2 | z1 + z2} models the response \code{y} as function of one stimulus with features \code{x1}, \code{x2} and one stimulus with features \code{z1}, \code{z2}. Horizontal lines (\code{|}) separate different stimuli.}

\item{class}{A formula, variable in \code{data} that has the class feedback, e.g. \code{~ category}. \code{NA}s are interpreted as trials without feedback (partial feedback, see details).}

\item{data}{A data frame.}

\item{options}{(optional) A list to change the parameter estimation, see \code{\link[=cm_options]{cm_options()}} or the section Options below.}

\item{similarity}{(optional) A string, similarity function, currently only \code{"minkowski"}.}

\item{...}{(optional) Other arguments from other functions, ignored.}

\item{criterion}{A formula, variable in \code{data} that has the criterion feedback, e.g. \code{~ value} \code{NA}s are interpreted as trials without feedback (partial feedback, see details).}

\item{mode}{(optional) A string, the response mode, either \code{discrete} or \code{continuous}, can be abbreviated. If missing, will be inferred from \code{criterion}.}
}
\value{
Returns a cognitive model object, which is an object of class \href{Cm}{cm}. A model, that has been assigned to \code{m}, can be summarized with \code{summary(m)} or \code{anova(m)}. The parameter space can be viewed using \code{parspace(m)}, constraints can be viewed using \code{constraints(m)}.

Returns the fitted model which has class \code{cm}. A model called \code{M} can be viewed with \code{summary(M)}, or \code{anova(M)}.
}
\description{
\code{ebm()} fits an exemplar-based model
\itemize{
\item \code{gcm()} fits a generalized context model, aka. exemplar model, for discrete responses (Medin & Schaffer, 1978; Nosofsky, 1986)
\item \code{ebm_j()} fits an exemplar-based judgment model for continuous responses (Juslin et al., 2003)
}
}
\details{
\code{ebm_j()} calls \link{ebm} with \code{mode = "continuous"}.

\code{gcm()} \link{ebm} with \code{mode = "discrete"}.
}
\section{Options}{

The following can be passed in one list, e.g. \code{options = list(lb = c(k = -10))}.

\describe{
\item{\code{lb}}{Named numeric vector, sets the lower parameter bounds;
\code{lb = c(k = -10)} lets a parameter \emph{k} start at -10.}
\item{\code{ub}}{Named numeric vector, sets the upper parameter bounds:
\code{ub = c(k = 10)} lets a parameter \emph{k} go until 10.}
\item{\code{start}}{Named numeric vector, sets the start values of parameter:
\code{start = c(k = 5)} lets a parameter \emph{k} start at 5.}
\item{\code{fit}}{Logical (default \code{TRUE}), \code{fit = FALSE} disables parameter
estimation. Useful for testing models.}
\item{\code{fit_measure}}{A string (default \code{"loglikelihood"}), the goodness
of fit measure that is optimized during parameter estimation.
Can be one of the \code{types} in the function [cognitiveutils::gof()\verb{: }"loglikelihood"\verb{. Uses a binomial PDF in models with discrete  data. Uses a normal PDF \eqn{N(\mu, \sigma)} in models with  continuous data:  \eqn{\mu}=predictions, \eqn{\sigma}=constant, estimated as additional free paramter. To change the PDF set }fit_args = list(pdf = "")}
\item{\code{fit_args}}{Named list, options for parameter estimation. Can be
arguments to the function (gof())\code{\link[cognitiveutils:gof]{cognitiveutils::gof()}}.
\code{list(pdf = "truncnorm", a = 0, b = 1)} changes the PDF
in the log likelihood to a truncated normal between 0 and 1,
\code{list(pdf = "multinom")} changes it to a multinomial PDF.
\code{list(grid_offset = .01)} offsets the parameter in a grid search
by 0.01 from the parameter boundaries, \code{list(nsteps = 10)} defines
10 steps for each parameter in the regular grid in the grid search.}
\item{\code{fit_data}}{A data frame, the data to estimate the model parameters
from. Needed if the data for the parameter estimation differs from
the data in the main \code{data} argument in a model.}
\item{\code{solver}}{A string, the optimizer for the parameter estimation. Run
\code{cm_solvers()} to list all solvers. Can be \code{"grid"}, \code{"solnp"}
\code{"optimx"}, \code{"nloptr"}, \code{"nlminb"} and others from \link{ROI}. Can be
\code{c("grid", "xxx")}: a grid-plus-optimization: A grid search, followed
by an optimization with xxx using the \emph{n}
best solutions as start values in the optimization; the overal best
parameter set wins; and \emph{n} can be set in \code{solver_args$nbest}
Changing the solver may cause warnings and ignored parameter bounds.}
\item{\code{solver_args}}{A named list, additional arguments passed directly
to the solver function, see the pages of the solver to see which
arguments the solver function has. For example:
\code{list(offset = 0.01)} offset the parameters from their boundaries
when \code{solver = "grid"}.
\code{list(nsteps = 10)} uses 10 steps for each parameter in the regular
grid, for \code{solver = "grid"}), \code{list(nbest = 3)} uses the 3 best
parameter sets from the grid search as starting values in a
grid-plus-optimization solver, for \code{solver = c("grid", "xxx")}.
\code{list(control = )} control arguments in the solver
(solnp)\code{\link[Rsolnp:solnp]{Rsolnp::solnp()}} and the
(ROI solvers)\link{https://rdrr.io/cran/ROI/man/ROI_solve.html}}
}
}

\section{Parameter Space}{

The parameters of the model are listed here, LB and UB = inclusive lowr and upper parameter bounds, Start Value = starting value for fitting.
\tabular{lrcllr}{\verb{   }
\strong{Name} \tab \verb{   }\strong{LB} \tab  \strong{-} \tab \strong{UB}\verb{   } \tab \strong{Description} \tab \strong{Start Value} \cr\verb{   }
\code{f1, f2} \tab 0 \tab-\tab 1 \tab Attention weights, sum to 1. Note: parameter names are equal to the LHS of \code{formula} \tab 1 / n features \cr\verb{   }
\code{lambda} \tab 0.001 \tab-\tab 10 \tab Sensitivity, higher values increase the discriminability in the psychological space \tab 0.5 \cr\verb{   }
\code{q} \tab 0 \tab-\tab 2 \tab Exponent in the distance metric, 1 yields city-block, 2 yields Euclidean metric \tab 1.5 \cr\verb{   }
\code{r} \tab 0 \tab-\tab 2 \tab Exponent in the decay functio, 1 yieldes = exponential decay, 2 yields gaussian decay \tab 1.5 \cr\verb{   }
\code{b0, b1} \tab 0 \tab-\tab 1 \tab (only for discrete responses) Response bias, sum to 1, names are \code{b} + \code{unique(class)}.\tab 1 / n classes
}
}

\section{Partial Feedback}{

How are \code{NA} values in \code{class} and \code{criterion} handeled? The model takes \code{NA} values in the class or criterion variable as trials without feedback, in which a stimulus was shown but no feedback about the class or criterion was given (partial feedback paradigm). The model predicts the class or criterion for such trials without feedback based on the previous exemplars for which feedback was shown. The model ignores the trials without feedback in the prediction of the subsequent trials.
}

\examples{
# Make some fake data
D <- data.frame(f1 = c(0,0,1,1,2,2,0,1,2),     # feature 1
                f2 = c(0,1,2,0,1,2,0,1,2),     # feature 2
                cl = c(0,1,0,0,1,0,NA,NA,NA),  # criterion/class
                 y = c(0,0,0,1,1,1,0,1,1))     # participant's responses

M <- gcm(y ~ f1+f2, class= ~cl, D, fix="start",
         choicerule = "none")                  # GCM, par. fixed to start val.

predict(M)                                     # predict 'pred_f', pr(cl=1 | features, trial)
M$predict()                                    # -- (same) --
summary(M)                                     # summary
anova(M)                                       # anova-like table
logLik(M)                                      # Log likelihood
M$logLik()                                     # -- (same) --
M$MSE()                                        # mean-squared error
M$npar()                                       # 7 parameters
M$get_par()                                    # parameter values
M$coef()                                       # 0 free parameters


### Specify models
# -------------------------------
gcm(y ~ f1 + f2, class = ~cl, D, 
    choicerule = "none")                          # GCM (has bias parameter)
ebm(y~f1+f2, criterion=~cl, D, mode="discrete",
    choicerule = "none")                          # -- (same) --
ebm_j(y ~ f1 + f2, criterion = ~cl, D)              # Judgment EBM  (no bias par.)
ebm(y~f1+f2, criterion=~cl, D, mode="continuous")   # -- (same) --


### Specify parameter estimation
# -------------------------------
gcm(y~f1+f2, ~cl, D, fix="start", 
    choicerule = "none")                        # fix all par to start val. 
gcm(y~f1+f2, ~cl, D, fix=list(b0=0.5, b1=0.5),
     choicerule = "none")                       # fix 'bias' par. to 0.5, fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(f1=0.9,f2=0.1),
     choicerule = "none")                       # fix attention 'f1' to 90 \%  f1 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(q=2, r=2),
     choicerule = "none")                      # fix 'q', 'q' to 2 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(q=1, r=1),
     choicerule = "none")                      # fix 'q', 'r' to 1 & fit 5 par
gcm(y~f1+f2, ~cl, D, fix=list(lambda=2),
     choicerule = "none")                      # fix 'lambda' to 2 & fit 6 par
ebm_j(y ~ f1 + f2, D)

}
\references{
{Medin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. \emph{Psychological Review, 85}, 207-238. \url{http://dx.doi.org/10.1037//0033-295X.85.3.207}}

{Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. \emph{Journal of Experimental Psychology: General, 115}, 39-57. \url{http://dx.doi.org/10.1037/0096-3445.115.1.39}}

{Juslin, P., Olsson, H., & Olsson, A.-C. (2003). Exemplar effects in categorization and multiple-cue judgment. \emph{Journal of Experimental Psychology: General, 132}, 133-156. \url{http://dx.doi.org/10.1037/0096-3445.132.1.133}}

The model can predict new data (\code{predict(M, newdata = ....)}), and this is how it works:
\itemize{
\item{If \code{newdata}'s \code{criterion} has only \code{NA}s, the model predicts using the old data (the originally-supplied \code{data} argument) as exemplar-memory. Parameters are not re-fit.}
\item{If \code{newdata}'s' \code{criterion} has also non-\code{NA}s, the model predicts the first new data row using the old data, but predictions for subsequent new data use also the criterion in new data. In other words, exemplar memory is \emph{extended} by exemplars in new data for which a criterion exists. Parameters are not re-fit.}
}
}
\seealso{
Other cognitive models: 
\code{\link{Cm}},
\code{\link{baseline_const_c}()},
\code{\link{bayes}()}
}
\concept{cognitive models}
