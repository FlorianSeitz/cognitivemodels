% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/choicerules.R
\name{softmax}
\alias{softmax}
\alias{epsilon_greedy}
\alias{epsilon}
\alias{argmax}
\alias{luce}
\title{Decision rule models (action selection, choice rules)}
\usage{
softmax(formula, data, fix = list(), options = NULL)

epsilon_greedy(formula, data, fix = list(), options = NULL)

epsilon(formula, data, fix = list(), options = NULL)

argmax(formula, data, ...)

luce(formula, data, ...)
}
\arguments{
\item{formula}{A \code{\link{formula}} like responses ~ action-values (e.g.,  \code{y ~ a | b}). \emph{Note}, that bars (\code{|}) separate different actions.}

\item{data}{A data.frame containing \code{formula}'s variables.}

\item{fix}{(optional) Parameter constraints. Can be \code{"start"} or a list with \code{parname=value}-pairs. Parameter names see below under "parameter space".
\itemize{
\item{\code{"start"} constrains all available parameters to their starting values. Useful for model testing.}
\item{\code{parname = 0.5} constrains a parameter to 0.5.}
\item{\code{parname = "p2"} constrains a parameter to another model parameter \code{p2}.}
\item{\code{parname = NA} tells the model to omit a parameter, if possible.}
}}

\item{options}{(optional) Options to control the parameter fitting methods, see the "Options" section of \code{\link{cogscimodel}}.}
}
\value{
An object of class R6 holding a model.
}
\description{
Models of action selection: \code{softmax()} is soft-max, \code{argmax()} is maximizing, \code{epsilon_greedy()} is epsilon-greedy, \code{epsilon()} is probabilistic-epsilon, \code{luce()} is proportional (Luce's rule).
}
\details{
This is how the model predicts and treats observations:
\itemize{
   \item{For \code{y ~ a} it predicts 1 response column (\code{pred_a}) and interprets observations in \code{y} as \emph{0 = \bold{not}-a}, \emph{1 = a}}
   \item{For \code{y ~ a | b} it predicts 2 response columns (\code{pred_a, pred_b}) and interprets observations in \code{y} as \emph{0 = a}, \emph{1 = b}}
\item{For \code{y ~ a | b | c} it predicts 3 response columns (\code{pred_a, pred_b, pred_c}) and interprets observations in \code{y} as \emph{0 = a}, \emph{1 = b}, \emph{2 = c}}
\item{etc.}
}

\code{softmax()} picks action \eqn{i} with \eqn{exp[v(i)]/\tau / \sum exp[v]/\tau}. It equals a logistic action selection for binary actions and \eqn{\tau=1}.

\code{epsilon_greedy()} picks the best action with probability \eqn{1 - \epsilon}, and with \eqn{\epsilon} it picks randomly from all actions, including the best.

\code{epsilon()} picks action \eqn{i} with probability \eqn{(1 - \epsilon)*p(i)} and with \eqn{\epsilon} it picks randomly from all actions. For \eqn{\epsilon = 0} it gives \eqn{p(i)}, that is the  original probabilistic policy.

\code{argmax()} picks the best action with probability \eqn{1}. Ties are broken by random predictions: it gives \eqn{1 / m} if \eqn{m} actions have equal values.

\code{luce()} picks action \eqn{i} with probability \eqn{v(i) / \sum v}.
}
\section{Parameter Space}{

\tabular{llrcllr}{\tab \verb{   }\strong{Name} \tab \verb{    }\strong{LB} \tab  \strong{-} \tab \strong{UB}\verb{    } \tab \strong{Description} \tab \strong{Start Value}\cr
\code{softmax()} \tab \verb{   }\code{tau} \tab  0.0001 \tab  - \tab  10 \tab  Temparature, small \code{tau} yields maximizing, high yields randomizing \tab  1\cr
\code{epsilon()} \tab \verb{   }\code{eps} \tab  0 \tab  - \tab  1 \tab  Exploration probability; small \code{eps} yields original input, high yields randomizing \tab  0.5 \cr
\code{epsilon_greedy()} \tab \verb{   }\code{eps} \tab  0 \tab  - \tab  1 \tab  Exploration probability; small \code{eps} yields maximizing, high yields randomizing \tab  0.5
}
}

\examples{
# Make some fake data
D <- data.frame(a = c(.3,.4,.5),       # value of option A
                b = c(.7,.6,.5),       # value of option B
                y = c(1,1,0))          # respondent's choice (0=A, 1=B)

M <- softmax(y ~ a | b, D, c(tau=1))   # creates soft-max model

predict(M)                             # predict action selection
M$predict()                            # -- (same) --
summary(M)                             # summarize
anova(M)                               # anova-like table
coef(M)                                # free parameter (NULL)
M$get_par()                            # fixed parameter (tau = 1)
M$npar()                               # 1 parameter
M$MSE()                                # mean-squared error
logLik(M)                              # log likelihood


### Parameter specification and fitting
# -------------------------------------
softmax(y ~ a | b, fix = "start")      # fix 'tau' to its start value
softmax(y ~ a | b, fix = c(tau=0.2))   # fix 'tau' to 0.2
softmax(y ~ a | b)                     # fit 'tau' to data y


### The different choice rules
# -----------------------------------
softmax(y ~ a | b, fix = c(tau=0.5))   # fix 'tau' to 0.5
softmax(y ~ a | b)                     # fit 'tau' to y
epsilon_greedy(y~a | b, D, c(eps=0.1)) # fix 'eps' to 10 \%
epsilon_greedy(y~a | b, D )            # fit 'eps' to y
epsilon(y ~ a | b, D, c(eps=0.1))      # fix 'eps' to 0.1
epsilon(y ~ a | b, D)                  # fit 'eps' to y
argmax(y ~ a | b, D)                   # arg max
luce(y ~ a | b, D)                     # Luce's choice rule
}
\references{
Sutton, R. S., & Barto, A. G. (2018). \emph{Reinforcement Learning: An Introduction (2nd Ed.)}. MIT Press, Cambridge, MA. \url{http://incompleteideas.net/book/the-book-2nd.html}

Luce, R. D. (1959). On the possible psychophysical laws. \emph{Psychological Review, 66(2)}, 81-95. \url{https://doi.org/10.1037/h0043178}
}
\author{
Jana B. Jarecki
}
\concept{Choicerules for cognitive models}
